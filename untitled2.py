# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_i0dPaIVh0RaOcibX-0xii5rayI5cMSi
"""

#CNN
import tensorflow as tf

mnist=tf.keras.datasets.mnist
#Modified National Institution of standards and Technology database
#it has a cllection of greyscall images with handwritten digits from 0  to 9

print(mnist)

(x_train,y_train),(x_test,y_test)=mnist.load_data()
x_train,x_test=x_train/255.0,x_test/255.0

model=tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28,28)),
    tf.keras.layers.Dense(128,activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10,activation='softmax'),
    ])

#x_train,x_test=x_train/255.0,x_test/255.0
#1.) why to divide by 255
# when we are working with image data,the pixel values are integers in the rsange of [0,255].so,dividig it by 255.0 scales these values to the range of
# [0,1]
#Working with the smaller rallies increase the stability of optimization algorithm
#tf.keras.layers.dense(128, activation='relu'),
# 2.) why 128
#It is a specific number of neurons or units in the dense layer
#relu  --> Recieved Linear Unit,It helps to add non-linearity  to our algorithm

model.compile(optimizer='adam',loss="sparse_categorical_crossentropy",metrics=['accuracy'])

model.fit(x_train,y_train)
model.fit(x_train,y_train,epochs=5)

test_loss,test_accuracy=model.evaluate(x_test,y_test)

print(test_loss)
print(test_accuracy)

#Activation function
#Relu
#It is one of the most widely used activation function,It replaces all negative values with zero, leaving
#positive values unchanged

#signoid
#signoid reduces the output b/w 0 and 1 ,making it suitable for binary classification problems

#Tanh (Hyperbolic Tangent)
#It reduces the b/w -1 and 1

#softmax
#It is mostly used in output layer
#leaku Relu
#It is a another varient of Relu that allows a small positive gradient for negative value just to avoid dead neurons

#Project:1
#Convert image to greyscall using CNN

import tensorflow as tf
from tensorflow.keras import layers,models
from tensorflow.keras.preprocessing.image import load_img,img_to_array
import matplotlib.pyplot as plt
import numpy as np

#Load the RBG image
image_path ="/content/IMG_20230602_155636.jpg"
original_image=load_img(image_path,target_size =(224,224))
original_array =img_to_array(original_image)
original_array=original_array/225.0
print(original_array)

original_array.shape

plt.figure(figsize=(6,6))
plt.subplot(1, 2, 1)
plt.title("original image")
plt.imshow(original_array)

import numpy as np
import pandas as pd
#convert image to grey scaler
model = models.Sequential()

#sequentials() --->it allows us to create a linear stack of layers in a
#neural network
#you can add one layer of neural network  at a time and each layer has
#connection only to the previous  and next layer
model.add(layers.Conv2D(1, (3, 3 ), activation='relu', input_shape=(224, 224, 3)))
#Conv2d--> It represent 2D conventional Layer
#conventional layer-->used to perform element wise
#multiplication or addition or division etc..
#layers.Conv2D(1) -->1 is used to mention the number of filters in the conventional layer
#(3,3) --> set the size of filter
#(224,224,3) -->224x224 --> pixels with 3 color channel(RGB)
model.add(layers.MaxPooling2D((2,2)))
#MaxPooling()--> it is used to set the dimensions  of input data
#and extract important features from conv2d layer

model.summary()

#to reshape the image

input_image=np.expand_dims(original_array,axis=0)
greyscale=model.predict(input_image)

plt.figure(figsize=(6,6))
plt.subplot(1, 2, 1)
plt.title("original image")
plt.imshow(original_array)

plt.figure(figsize=(6,6))
plt.subplot(1, 2, 1)
plt.title("original image")
plt.imshow(np.squeeze(greyscale),cmap='gray')

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import style

#model_selection
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import LabelEncoder

#preprocess
from keras.preprocessing.image import ImageDataGenerator

from keras import backend as K
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop
from keras.utils import to_categorical

from keras.layers import Dropout, Flatten,Activation
from keras.layers import Conv2D,MaxPooling2D,BatchNormalization

import tensorflow as tf
import random as rn

import cv2
import numpy as np
from tqdm import tqdm
import os
from random import shuffle
from zipfile import ZipFile
from PIL import Image

X=[]
Z=[]
IMG_SIZE=150

FLOWER_DAISEY_DIR='/content/drive/MyDrive/archive (4)/train/daisy'
FLOWER_SUNFLOWER_DIR='/content/drive/MyDrive/archive (4)/train/sunflower'
FLOWER_TULIP_DIR='/content/drive/MyDrive/archive (4)/train/tulip'
FLOWER_DANDI_DIR='/content/drive/MyDrive/archive (4)/train/dandelion'
FLOWER_ROSE_DIR='/content/drive/MyDrive/archive (4)/train/rose'

def assign_label(img,flower_type):
  return flower_type

#tqdm-->It creates a progress bar from the loop
def make_train_data(flower_type, DIR):
  for img in tqdm(os.listdir(DIR)):
    label=assign_label(img, flower_type)
    path=os.path.join(DIR, img)
    img=cv2.imread(path, cv2.IMREAD_COLOR)
    img=cv2.resize(img, (IMG_SIZE, IMG_SIZE))
    X.append(np.array(img))
    Z.append(str(label))

make_train_data('Daisy', FLOWER_DAISEY_DIR)
print(len(X))

make_train_data('Sunflower', FLOWER_SUNFLOWER_DIR)
print(len(X))

make_train_data('Tulip', FLOWER_TULIP_DIR)
print(len(X))

make_train_data('Dandelion', FLOWER_DANDI_DIR)
print(len(X))

make_train_data('Rose', FLOWER_ROSE_DIR)
print(len(X))

fig, ax = plt.subplots(5, 2)
fig.set_size_inches(15, 15)

for row in range(5):
  for col in range(2):
    l= rn.randint(0, len(Z))
    ax[row, col].imshow(X[l])
    ax[row, col].set_title("Flower: "+Z[l])

plt.tight_layout()



